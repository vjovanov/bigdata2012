Cluster computing systems today impose a trade-off between generality, performance and productivity. Hadoop and Dryad enforce low level programs that are tedious to write but easy to optimize. Systems like Dryad/LINQ and Spark allow concise modelling of user programs but do not apply relational optimizations. Pig and Hive restrict the language to achieve relational optimizations but complex programs can be hard to express without user extensions. However, these extensions are cumbersome to write and disallow program optimizations.

This paper presents the big data procesing library \tool. It uses deep language embedding in Scala, staged execution and explicit side effect tracking to analyze the structure of user programs. This analysis is used to apply early projection insertion, that eliminates unused data, and code motion together with operation fusion to highly optimize the performance critical path of the system. Language embedding and high level interface make \tool programs expressive and resemble regular Scala code. Modular design and the library approach allow users to extend \tool with abstract and at the same time high performance modules. Through the modular code generation scheme \tool can execute programs on both Spark and Hadoop. On Spark we achieve 149\% speedups while on Hadoop we achieve 59\% speedups. 