Cluster computing systems today impose a trade-off between generality, performance and productivity. Hadoop and Dryad force programmers to write low level programs that are tedious to compose but easy to optimize. Systems like Dryad/LINQ and Spark allow concise modelling of user programs but do not apply relational optimizations. Pig and Hive restrict the language to achieve relational optimizations, making complex programs hard to express without user extensions. However, these extensions are cumbersome to write and disallow program optimizations.

We present a the big data processing library called \tool. It uses deep language embedding in Scala, staged execution and explicit side effect tracking to analyze the structure of user programs. This analysis is used to apply early projection insertion, which eliminates unused data, and code motion, together with operation fusion to highly optimize the performance critical path of the system. The language embedding and a high-level interface make \tool programs expressive and resembling regular Scala code. Modular design and a library approach allow users to extend \tool with modules, which specify an abstract interface and how to generate high performance code for it. Through the modular code generation scheme, \tool can execute programs on both Spark and Hadoop. On Spark we achieve speedups of up to 149\% over na√Øve implementations, while on Hadoop they can reach up to 59\%.