Cluster computing systems today impose a trade-off between generality,
performance and productivity. Hadoop and Dryad force programmers to write low
level programs that are tedious to compose but easy to optimize. Systems like
Dryad/LINQ and Spark allow concise modeling of user programs but do not apply
relational optimizations. Pig and Hive restrict the language to achieve
relational optimizations, making complex programs hard to express without user
extensions. However, these extensions are cumbersome to write and disallow
program optimizations.

We present a distributed batch data processing framework called \tool.
\tool uses deep language embedding in Scala, multi-stage programming and
explicit side effect tracking to analyze the structure of user programs. The
analysis is used to apply projection insertion, which eliminates unused data, as
well as code motion and operation fusion to highly optimize the performance
critical path of the program. The language embedding and a high-level interface
allow \tool programs to be both expressive, resembling regular Scala code, and
optimized. Its modular design allows users to extend \tool with modules that
produce highly performant code. Through a modular code generation scheme, \tool
can generate programs for both Spark and Hadoop. Compared with na\"{\i}ve
implementations we achieve 143\% speedups on Spark and 126\% on Hadoop.
