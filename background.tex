\section{Background}
\label{sec:background}
In this section we explain language virtualization in Scala, Lightweight Modular
Staging (LMS) library \cite{rompf_lightweight_2010, rompf_lightweight_2012},
optimizations in LMS and distributed batch processing frameworks
relevant for this paper.
% example of LMS optimizations. Missing is DCE on unused fields.
\begin{figure*}
  \begin{subfigure}[b]{.5\linewidth}
\begin{lstlisting}
def parse(st: Rep[String]) = {               
  val sp = st.split("\\s")
  Complex(Float(sp(0)), Float(sp(1)))
}
val x = new Array[Complex](input.size)
for (i <- 0 to input.size) {
  x(i) = parse(input(i)) 
}
for (i <- 0 to x.size) {
  if (x(i).im == 0) println(x(i).re) 
}
\end{lstlisting}
    \caption{Original program}
    \label{lst:original}
  \end{subfigure}
  \begin{subfigure}[b]{.5\linewidth}
\begin{lstlisting}
val size = input.size
val x = new Array[Complex](size)
for (i <- 0 to size) {
  val pattern = new Pattern("\\s") 
  val sp = pattern.split(input(i))
  x(i) = Complex(Float(sp(0)), Float(sp(1))) 
}
for (i <- 0 to x.size) {
  if (x(i).im == 0) println(x(i).re) 
}
\end{lstlisting}
    \caption{CSE and inlining} 
    \label{lst:cse-inline}
  \end{subfigure}
  \begin{subfigure}[b]{.5\linewidth}
\begin{lstlisting}
val size = input.size
val re = new Array[Float](size)
val im = new Array[Float](size)
for (i <- 0 to size) {
  val pattern = new Pattern("\\s") 
  val sp = pattern.split(input(i))
  re(i)  =  Float(sp(0))
  im(i) = Float(sp(1)) 
}
for (i <- 0 to size) {
  if (x(i).im == 0) println(x(i).re) 
}
\end{lstlisting}
    \caption{AoS $\rightarrow$ SoA}
    \label{lst:aos-soa}
  \end{subfigure}
\begin{subfigure}[b]{.5\linewidth}
\begin{lstlisting}
val size = input.size
val pattern = new Pattern("\\s")
 
for (i <- 0 to size) {
  val sp = pattern.split(input(i))
  val im = Float(sp(1))
  if (im == 0)  {
    val re = Float(sp(2))
    println(re)
  }
}
\end{lstlisting}
    \caption{Loop fusion and code motion}
    \label{lst:fusion-motion}
  \end{subfigure}
  \caption{Step by step optimizations in LMS.}
  \label{lst:step-by-step-lms}
\end{figure*}

% language virtualization
\subsection{Virtualized Scala}
\label{subsec:virtualized-scala}
\tool is written in an experimental version of Scala called Virtualized Scala
\cite{moors_scala-virtualized_2012} which provides facilities for deep embedding
of domain-specific languages (DSLs). Deep embedding is achieved by translating
regular language constructs like conditionals, loops, variable declarations and
pattern matching to regular method calls. For example, for the code \code{if (c)
a else b}, the conditional is not executed but instead a method call is issued
to the method \code{__ifThenElse(c, a, b)}. In case of deeply
embedded DSLs this method is overridden to create an intermediate
representation (IR) node that represents the \code{if} statement.

In Virtualized Scala, all embedded DSLs are written within DSL scopes. These
special scopes look like method invocations that take one by name parameter
(block of Scala code). They get translated to the complete specification of DSL
modules that are used in a form of a Scala trait mix-in
composition\footnote[1]{Scala's support for multiple inheritance}. For example:
\code{jetDSL\{ \\\\ dsl code \}} gets translated into:
\scode{new JetDSL \{ def main()\{...\}\}} 
This makes all the DSL functionality defined in JetDSL visible in the body of
the by name parameter passed to the jetDSL method.
Although modified, Virtualized Scala is fully binary compatible with Scala and
can use all existing libraries.


% LMS
\subsection{Lightweight Modular Staging}
\label{subsec:lightweight-modular-staging}

% lms basics
\tool is built upon the Lightweight Modular Staging (LMS) library. LMS utilizes
facilities provided by Virtualized Scala to build a modular compiler
infrastructure for developing staged DSLs. It represents types in a DSL with the
polymorphic abstract data type \code{Rep[T]}. A term inside a DSL scope that has
a type \code{Rep[T]} declares that once the code is staged, optimized, and
generated, the actual result of the term will have type \code{T}. Since
\code{Rep[T]} is an abstract type, each DSL module can specify concrete
operations on it, which are used for building the DSL's intermediate
representation.

Since Scala's type system supports type inference and implicit conversions, most
of the \code{Rep[T]} types are hidden from the DSL user. This makes the DSL code
free of type information and makes the user almost unaware of the \code{Rep}
types. The only situation where \code{Rep} types are visible is in
parameters of methods and fields of defined classes. In our experience
with writing DSLs, \code{Rep} types do not present a problem but gathering
precise and unbiased information on this topic is difficult.

% modularity through scalable components abstraction.
The modular design of LMS allows the DSL developer to arbitrarily compose the
interface, optimizations and code generation of the DSL. Module inclusion is
simply done by mixing Scala traits together. The correctness of the composition
and missing dependencies are checked by the type system. Code generation for a
DSL is also modular, so the effort is almost completely spent on domain-specific
aspects. LMS provides implementations for most of the Scala constructs and the
most common libraries. This allows to make DSLs that are fairly general --
comparable to standard Scala.

% LMS effects
Unlike Scala, which does not have an effect tracking mechanism, LMS provides
precise information about the effect for each available operation. The DSL
developer needs to explicitly specify the effects for each DSL operation he
introduces. The LMS effect tracking system then calculates the effect summary
for each basic block in the DSL code. This allows the optimizer to apply code
motion on the pure (side effect free) parts of the code. All implementations for
standard library constructs that LMS provides such as strings, arrays, loops and
conditionals, already include effect tracking.

% workflow + shallow embedding
LMS builds a complete intermediate representation of the code, optimizes it, and
then generates optimized code for the chosen target language. The generated code
has then to be compiled itself, so it can be invoked. If the compilation
delay is deemed inappropriate for a certain use case, it is possible to execute
the code directly in Scala. A shallow DSL embedding can be achieved this way,
instead of building the IR.

% println dsl 
\begin{lstlisting}[name=code, caption=Example of the DSL module used for
printing strings.
,captionpos=b, label=lst:println_dsl, float=t] 
// creates the println statement in the IR 
trait PrintlnExp extends BaseExp {
  def println[T](st: Rep[String]) =
    reflectEffect(PrintlnNode(st)) 
  }
}

// code generator for println IR
trait PrintlnGen extends ScalaGen {
  def emit(node: Rep[Any]) = node match {
    case PrintlnNode(str) =>
      println("println("+str+")")
  }
}
\end{lstlisting}

 
In Listing \ref{lst:println_dsl}, we show a simplified version of a DSL module
for printing strings. In trait \code{PrintlnExp} we define how the
\code{println} operation builds the IR node. The \code{reflectEffect} method
defines that the \code{println} method has global side effects which signals the
compiler that it can not be reordered with respect to other globally effectful
statements or be moved across control structures. In the \code{PrintlnGen}
trait, we define how the code for \code{println} is generated for Scala.

Until now, LMS has been successfully used by Brown et al. for heterogeneous
parallel computing in project Delite \cite{brown_heterogeneous_2011, dsl11} and by
Kossakowski et al. for a JavaScript DSL \cite{greg}.

\subsection{LMS Optimizations}
\label{subsec:lms-optimizations}
% common compiler optimizations
When writing DSLs, the DSL authors can exploit their domain knowledge to apply
high level optimizations and program transformations. Afterwards, the program is
usually lowered to a representation closer to the actual generated code. LMS
provides a set of common optimizations for the lowered code, which are: common
subexpression elimination (CSE), dead code elimination (DCE), constant folding
(CF) and function inlining. LMS also applies code motion, which can either:
\emph{i)} move independent and side effect free statements out of hot loops
\emph{ii)} move statements that are used inside conditionals but defined
outside closer to their use site.

% structures
Another interesting optimization is the transformation of an array of structural
types to a structure of arrays (AoS $\rightarrow$ SoA), each containing only
primitive fields. This transformation removes unnecessary constructor
invocations and enables DCE to collect unused fields of an structure. It can be
applied to built-in data structures like tuples as well as immutable
user-defined types. It is similar in effect to column-oriented storage in
databases and it gives great performance and memory footprint improvements.

% loop fusion
LMS also provides a very general mechanism for loop fusion that uses
standard loops as the basic abstraction. It is better than existing
deforestation approaches since it generalizes to loops and can apply both
vertical and horizontal fusion. In vertical fusion, the algorithm searches for
producer consumer dependencies among loops, and then fuses their bodies
together. In horizontal fusion, independent loops of the same shapes are fused
together and index variables are relinked to the fused loop's index variable.
Fusion greatly improves performance as it removes intermediate data structures
and uncovers new opportunities for other optimizations.

% Listing
In Listing \ref{lst:step-by-step-lms}, we present these optimizations on a
single example which parses an array of complex numbers and prints only the real parts of them.
Step \ref{lst:original}) shows the original program, step \ref{lst:cse-inline})
shows how CSE extracts \code{size} and inlining replaces \code{parse} and \code{split}
invocations with their bodies. In step \ref{lst:aos-soa}) the array $x$ of
complex numbers is split into two arrays of floating points.
In step \ref{lst:fusion-motion}) the loops are fused together, which then allows
code motion to move the constant pattern out of the loop and move the parsing of the
real component into the conditional. The intermediate arrays are then removed
by DCE.

\subsection{Distributed Batch Processing Frameworks}
\label{subsed:big-data-frameworks}

\tool generates Scala code for Crunch \cite{crunch} and Spark \cite{spark-nsdi}.
Crunch uses Hadoop as the execution engine and provides a collection-like
interface, backed by a MSCR implementation as presented by Dean et.\ al.\
\cite{chambers_flumejava:_2010}. Crunch is implemented in Java and provides a
rather low level interface, the user must for example specify the serialization
for user classes.

% explicit caching, low latency interactive data querying
Spark is a recent execution engine which makes better use of the cluster's
memory, explicitly allowing the user to cache data. This allows huge speedups on
iterative jobs which can reuse the same data multiple times compared to Hadoop.
Like Crunch it provides a collection-like interface, but it is more high-level.
Spark has support for multiple serialization frameworks and it also features a
REPL for low latency interactive data querying.
