\section{Discussion}
\label{sec:discussion}

% Generality of our approach
The language we provide is the same as Scala in its basic constructs, however it does not support all of the functionalities. The following functionalities are not available:  
\begin{itemize}
\item Projection optimization can only be applied to final immutable classes. This somewhat limits the language, but in large big data processing, data records are often not polymorphic. 
\item Polymorphism is supported only in a limited form. The limitation is that all possible implementations need to be known at staging time and currently it prevents optimization of the polymorphic method call.
%\item Polymorphism is supported only in a limited form. The limitation is that all possible implementations need to be known at staging time\todo{is this true?}.
\item The whole Scala library is not available in its DSL form. This however does not limit the user since JVM methods can be used in the native form, but they can not be optimized. Due to language embedding, for using JVM methods there is no boilerplate code. 
\end{itemize}

% Although it is not completely general   


% Code compilation and generation vs interactivity
One of the caveats of the staged DSL approach is that the program staging, compilation, generation and compilation of the generated code increases the startup time for the task. For the benchmarks we have evaluated that this process takes from 4 to 14 seconds. Although this can be significant, it needs to be only done once on a single machine so we believe it is not a limiting factor for batch jobs.

The only case where compile time becomes relevant is with back-ends that support interactive data analytics, like the Spark framework. Spending more than a couple of seconds for compilation would affect the interactivity.

We see two ways to overcome issues with delay in execution:
\begin{itemize}
\item We can implement a version which does not generate the IR, but executes the original code straight away. In this case all optimizations we perform would be disabled but the user would gain original Spark interactivity. This feature is not implemented in \tool, but Kossakowski et al. \cite{greg} have done this for the Javascript DSL.

\item If optimizations are required,  we can build and optimize the intermediate representation after each user input. This gives the compiler time to do the work while the last command is being typed. The overall delay in this case the delay would be $delay = IR\_building - user\_delay) + generated\_code\_compilation$. Since we did not optimize the compiler and do not have data about the time it takes to do interactive commands we can not speculate on the final result.
\end{itemize}

% Job specific tuning 
Each job requires a framework specific configuration for its optimal execution (e.g. the number of mappers, reducers, buffer sizes etc.). Our current API does not include tuning of these parameters, but in the future work we want to introduce a configuration part of the DSL to unify configuration of different backends.
With the current programming model it is not possible to tune these parameters in a unified way. 
