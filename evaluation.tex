\section{Evaluation}
\label{sec:evaluation}


We evaluate \tool by implementing a regex heavy word count, TPCH Q12 and a Kmeans application. \\
The word count is very heavy on the mapper side and only requires one shuffle stage. As input we use TODO, 62 Gb of tab separated values. It can not benefit from field reduction, but still uses the generated parsing method. We use multiple regex and evaluate different steps of the optimization for these regexes. TPCH Q12 uses an expensive join, which can dominate the rest of the performance improvements easily. As we have not optimized joins beyond projection optimization, we also provide numbers for just the mapper phase of this query. We use 100 Gb of generated data as input. \\
We took a version of Sparks Kmeans application and ported it to our own language. This application can neither use field reduction nor can it really profit from loop fusion. We extended our DSL for this program with a highly optimized Vector class, which is compiled into an Array. As this program uses operations only available in spark, and it has been shown that spark outperforms hadoop by a large margin for it, we have only benchmarked it against the original spark implementation. As input we use synthetic data with 10 to 1000 dimensions, 100 centers and keeping the dimensions * points factor constant at TODO. \\
We performed our experiments on the Amazon EC2 Cloud, using 20 m1.large nodes as slaves and one as master. They each have 7.5 Gb Ram, 2 virtual cores with 2 EC2 units each, have TODO Gb of instance storage and provide high throughput. We measured up to 50 MB/s between such nodes. For the Hadoop experiments we created a cluster using whirr 0.7.1 and used Clouderas Hadoop distribution cdh3u4. Crunch  was in version 0.2.4 and scoobi in 0.4.0. For spark we used the mesos ec2 script to launch a cluster, and then used spark in version 048276799 for our tests. We did not configure hadoop at all, and for spark we only set the local temp folder and the default parallelism and increased the available memory, which were all essential for correctness or to get meaningful numbers. \\
For serialization we used our own generated Writables in both Crunch and Scoobi, since they seemed to outperform our kryo implementations and also scoobi's wireformat by a very thin margin. For Spark we used kryo. \\
The whole project is online at TODO. \\
We ran each program three times and display the average job time. The variances in job time were low. \\
