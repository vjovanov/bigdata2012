\subsection{Operation Fusion}
\label{sec:fusion}

In section \ref{sec:programming-model} we have shown that the \code{DList} provides declarative higher order operations. Anonymous functions passed to these operations do not share the same scope of variables. This reduces the number of opportunities for optimizations described in \ref{subsec:lms-optimizations}. Moreover, each piece of data needs to be read, passed to and returned from the higher order function. In both the push data-flow model and the pull model this enforces virtual method calls \cite{murray_steno:_2011} for each data record. To overcome this performance penalty we have implemented fusion of operations \code{map}, \code{flatMap} and \code{filter} through the underlying loop fusion algorithm described in section \ref{subsec:lms-optimizations}. 

The loop fusion optimization described in section \ref{subsec:lms-optimizations} supports horizontal and vertical fusion of loops as well as fusion of nested loops. Also, it provides a very simple interface to the DSL developer for specifying loop dependencies and for writing fusable loops. We decided to extend the existing mechanism to the \code{DList} operations although they are not strictly loops. We could have taken the path of Murray et al. in project Steno \cite{murray_steno:_2011} by generating an intermediate language which can be used for simple fusion and code generation. Also, we could use the Coutts et al. \cite{coutts_stream_2007} approach of converting \code{DList} to streams and applying equational transformation to remove intermediate results. After implementing the algorithm by reusing loop fusion we are confident that it required significantly less effort than reimplementing existing approaches.

Before fusion optimization, the program IR represents an almost one to one mapping to the operations in the programming model. Each operation is represented by the corresponding IR node which carries its data and control dependencies. On these IR nodes we first apply a lowering transformation which maps operations \code{map}, \code{flatMap} and \code{filter} to the corresponding loop representation. Described transformation is achieved by the program translation described in \ref{lst:lowering}. These rules introduce two new nodes: \emph{i)} $shape_dep$ that takes the place of the loop shape variable and carries the explicit information about its vertical predecessor and \emph{ii)} \code{$iterator_value$} that represents reading from an iterator of the preceding \code{DList}. The \code{yield} operation represents storing to the successor collection and is afterwards replaced by the bodies of fused loops.

% \begin{figure}
% \begin{lstlisting}
% DList nodes: $N$  
% Framework: $B = \{Spark, Scoobi, Crunch\}$ 
% Functions ($b \in B \land n \in N$): 
%   $multiple\_output(b) = b \ne Spark$
%   $fusible\_op(b, n) = map(n, \_) \lor filter(n, \_) \lor flatMap(n, \_) $
%   $fuse\_pred(b, n) = multiple\_output(b) \lor |succ(pred(n))| = 1$ 
%   $fusible(b, n) = fusible\_op(b, succ(n)) \lor$
%       $((fusible\_op(b, pred(n)) \land fuse\_pred(b, n))$
% \end{lstlisting}
% 
% 
% Lowering rules:
% \begin{lstlisting}
%          $b \in B$   $n \in N$     $fusible(b, n)$
% \end{lstlisting} 
% \vspace{-5pt}{\hspace{0mm}\rule{0.99\columnwidth}{0.25pt}}

% \end{lstlisting} 
% \begin{lstlisting}
%          $b \in B$   $n \in N$     $fusible(b, n)$
% \end{lstlisting} 
% \vspace{-5pt}{\hspace{0mm}\rule{0.99\columnwidth}{0.25pt}}
% \begin{lstlisting}

% \end{lstlisting}
% \begin{lstlisting}
%          $b \in B$   $n \in N$     $fusible(b, n)$
% \end{lstlisting} 
% \vspace{-5pt}{\hspace{0mm}\rule{0.99\columnwidth}{0.25pt}}
% \begin{lstlisting}

\begin{lstlisting}[name=code, caption=Lowering transformations., captionpos=b, label=lst:lowering, float=t]
$out = map(n, op) \rightarrow  loop(shape\_dep(n), \{$
  $yield(out, op(iterator\_value(n))\\$
$\})$
$out = filter(map, op) \rightarrow loop(shape\_dep(n), \{$
   $if (op(iterator\_value(in))$
     $yield(out, iterator\_value(in)) $
$\})$
$out = in.flatMap(op) \rightarrow loop(shape\_dep(n), \{$
  $w = op(iterator\_value(in))$
  $loop(w.size) \{yield(out, w(i))\}$
$\})$
\end{lstlisting}
% \caption{Lowering transformations.}
% \end{figure}


After the lowering transformation the loop fusion is applied. It vertically fuses pairs of loops until a fixed point is reached. In each fusion iteration all other LMS optimizations are applied as well. To avoid generating actual \code{while} loops we include a modified loop generation module for every back-end. This module emits the most general operation (equivalent of the Hadoop \code{Mapper} class) that the framework provides. With this approach we could also generate code directly for Hadoop MapReduce which would result in a single highly optimized loop per \code{Mapper}. After prototype experiments we concluded that the gain is not significant compared to using higher level back-ends. Therefore, as an alternative, we used Scoobi and Crunch.

Unlike MapReduce based back-ends, Spark's design uses the pull data-flow model, implemented through iterators. Generating code for the pull data-flow model from the loop based (push data-flow) model proved to be non-trivial. After evaluating different types of queues and array buffers we have decided to buffer intermediate results in a 4 MB array.