\section{Introduction}
\label{sec:introduction}

% Motivation -- three different types programming models and their problems
In the past decade numerous systems for big data cluster computing have been studied \cite{dean_mapreduce:_2008, yu_dryadlinq:_2008-1, olston_pig_2008-1, thusoo_hive_2010-1, zaharia_spark:_2010}. Programming models of these systems impose a trade-off between generality, performance and productivity. Systems like Hadoop \cite{_hadoop_????} provide a low level general purpose programming model that allows writing of fine grained and optimized code.  However, low level optimizations greatly sacrifice productivity \cite{chambers_flumejava:_2010}. Limited programming models like Pig latin \cite{olston_pig_2008-1} exploit domain knowledge to provide both good performance and productivity for large number of use cases. However, their generality is supported through user defined functions that are cumbersome and often disable optimizations. Finally, models like Spark \cite{zaharia_spark:_2010}, FlumeJava \cite{chambers_flumejava:_2010} and Dryad/LINQ \cite{yu_dryadlinq:_2008-1} provide high level operations and general purpose programming models but their performance is limited by glue code and lack of knowledge about program structure. Moreover, the number of existing programming models great and they significantly differ. This leads to low code portability and makes migration to a new framework very hard. 
% Should we mix in the multiple frameworks here.

% Trade-off - why cant there be both generality, performance and productivity
Above mentioned trade-off exists due to the imperative and polymorphic programming models commonly used today. Operations can have arbitrary side effects and the structure of the whole program (AST) is not available at runtime. This limits the compiler to highly optimize the code and leads to high level operations that are being glued together with inefficient abstractions like iterators and channels. Side effect free operations like date/time instantiation, regular expression compilation and high precision decimal numbers are often hidden by abstraction and recomputed in the hot path for each piece of data. Although domain specific approaches like Pig and SQL, with their narrow, side effect free, interface provide good optimizations they come with their own set of limitations. Interface is often too narrow for the problem leading to either invoking host language operations which are again hard to optimize or to abandoning the model completely. Moreover, there is the high overhead of learning the new language and the proper tool support in terms of debugging, type driven correctness checking and IDE is often limited.     

% Existing Solutions to achieve this
In the recent history there have seen several solutions that try to make programming big data systems efficient, productive and general at the same time. Steno \cite{murray_steno:_2011}, implements an innovative runtime code generation scheme that aims to eliminate iterators in Dryad/LINQ queries. It operates over flat and nested queries and produces a minimal number of while loops without any iterator calls. Manimal \cite{jahani_automatic_2011} and HadoopToSQL \cite{iu_hadooptosql:_2010} apply byte code analysis to extract information about accessed fields and selection branches to gain enough knowledge to apply common relational database optimizations for projection and selection. However, since these solutions use static byte code analysis they must be safely conservative which can lead to missed optimization opportunities.         

% Our implementation
This paper presents a new domain specific language \tool for big data processing that provides a high level interface similar to the Spark framework. Unlike Spark, \tool uses language virtualization \cite{moors_scala-virtualized_2012} and light weight modular staging (LMS)\cite{rompf_lightweight_2010} to get runtime access to the program structure. LMS enforces explicit effects tracking and provides DSL modules for majority of language constructs and often used libraries giving \tool great generality. \tool uses the program structure to apply optimizations like code motion, loop fusion, projection optimization and inlining to produce a highly optimized intermediate representation (IR). Highly optimized IR is then used by a thin code generation layer to produce code for arbitrary big data back-end making the user code easily portable. We implemented back-ends for Spark, Scoobi \cite{nicta_scoobi_2012} and Crunch \cite{_crunch_2012} that require from \todo{xxx to xxx} lines of code. The reason why back-ends are so thin is LMS modularity most of the code is reused so code generation back-ends can be so thin.
 
% Evaluation TODO(Stivo, vjovanov)

% Contributions 
\tool makes following contributions to the state of the art:    
\begin{itemize}

  \item We implement the \tool framework with a general, high level programming model for big data processing that applies relational and domain specific optimizations but does not come with the abstraction penalty and language restrictions.

  \item We introduce a novel projection optimization mechanism that operates across general program constructs like classes, conditionals, loops and user defined functions but does not rely safe assumptions.

  \item We show that language virtualization and modular staging permit framework specific optimizations together with common ones as well as user code portability through code generation for big data systems.

\end{itemize} 

% Sections
In section \ref{sec:background} we will provide background on LMS and language virtualization. In section \ref{sec:programming-model} we explain the interface and present simple program examples. Section \ref{sec:compiler} describes code motion, iterator fusion and other compiler optimizations. Section \ref{sec:field-reduction} explains the field reduction optimization. We evaluate \tool in \ref{sec:evaluation}, present related work in \ref{sec:related-work} and talk about future work and conclusions in \ref{sec:conclusion}.