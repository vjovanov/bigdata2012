\section{Introduction}
\label{sec:introduction}

% Motivation -- three different types programming models and their problems
In the past decade numerous systems for big data cluster computing have been studied \cite{dean_mapreduce:_2008, yu_dryadlinq:_2008-1, olston_pig_2008-1, thusoo_hive_2010-1, zaharia_spark:_2010}. Programming models of these systems impose a trade-off between generality, performance and productivity. Systems like Hadoop MapReduce\cite{_hadoop_????} and Dryad \cite{isard_dryad:_2007} provide a low level general purpose programming model that allows writing fine grained and optimized code.  However, low level optimizations greatly sacrifice productivity \cite{chambers_flumejava:_2010}. Limited programming models like Pig latin \cite{olston_pig_2008-1} exploit domain knowledge to provide both good performance and productivity for a large number of use cases. However, they support generality only through user defined functions that are cumbersome and difficult to optimize. Finally, models like Spark \cite{zaharia_spark:_2010}, FlumeJava \cite{chambers_flumejava:_2010} and Dryad/LINQ \cite{yu_dryadlinq:_2008-1} provide high level operations and general purpose programming models but their performance is limited by glue code between high level operations. Also, many relational optimizations are impossible due to the lack of knowledge about the program structure. 

% Trade-off - why cant there be both generality, performance and productivity
Above mentioned trade-off exists due to the imperative programming models, run-time binding of methods and open world assumptions commonly used in languages for big data processing. This limits compiler's ability to optimize the code. Inefficient abstractions like iterators and channels are not removed from the code that connects declarative operations. Side effect free operations like date/time instantiation, regular expression compilation and high precision decimal numbers are often hidden by abstraction and recomputed in the hot path for each piece of data. Domain specific approaches, like Pig and SQL, have a narrow and side effect free interface and provide good optimizations. However, they come with their own set of limitations. Their programming model is often too simple for the problem. This requires reverting to external language operations, which are again hard to optimize, or to abandoning the model completely. Moreover, there is the high overhead of learning the new language \todo{and proper tool support debugging, type driven correctness checking and IDE support.}
\todo{extensibility does not exist}
% Existing Solutions to achieve this
In the recent history there have been several solutions that try to make programming big data systems efficient, productive and general at the same time. Steno \cite{murray_steno:_2011} implements an innovative runtime code generation scheme that eliminates iterators in Dryad/LINQ queries. It operates over flat and nested queries and produces a minimal number of loops without any iterator calls. Manimal \cite{jahani_automatic_2011} and HadoopToSQL \cite{iu_hadooptosql:_2010} apply byte code analysis to extract information about unused data columns and selection conditions to gain enough knowledge to apply common relational database optimizations for projection and selection. However, since these solutions use static byte code analysis they must be safely conservative which can lead to missed optimization opportunities.

% general, restricted
% optimizations
% modular/extensible
% portable

% Our implementation
This paper presents a new domain specific language \tool for big data processing that provides a high level declarative interface similar to Dryad/LINQ and Spark. \tool builds upon language virtualization \cite{moors_scala-virtualized_2012} and lightweight modular staging \cite{rompf_lightweight_2010} (LMS) to provide  a programming model with few restrictions.  that uses the runtime program structure to apply first modular framework specific optimizations and than common ones like code motion, loop fusion, projection optimization and inlining. Result is a highly optimized intermediate representation (IR) which is then used by a thin code generation layer to produce code for multiple big data back-end. Having common optimizations separated from code generation layer makes supporting new backends  easy. This makes the user code easily portable between frameworks and their versions. 

%get runtime access to the program structure. LMS enforces explicit effects tracking and provides modules for majority of language constructs and often used libraries making \tool more general than Pig and SQL.

\todo{Evaluation}We implemented back-ends for Spark, Scoobi \cite{nicta_scoobi_2012} and Crunch \cite{_crunch_2012} and code generation layers range from 270 to 425 lines of code in size. 

% Contributions
\tool makes following contributions to the state of the art:    
\begin{itemize}


  \item \todo{link generality to optimizations}We implement the \tool framework for big data processing. \tool has a slightly restricted and high level programming model but still applies relational, domain specific and compiler optimizations to achieve high performance.


  \item We introduce a novel projection insertion algorithm that operates across general program constructs like classes, conditionals, loops and user defined functions, takes the whole program into account and does not rely on safe assumptions which lead to missed optimization opportunities.  


  \item We show that \tool allows easy language extension and code portability for big data frameworks. 

\end{itemize} 

% Sections
In section \ref{sec:background} we will provide background on LMS, language virtualization and big data frameworks. Then, in section \ref{sec:programming-model} we explain the programming model and present simple program examples. Section \ref{sec:field-reduction} explains the novel projection insertion optimization algorithm. Section \ref{sec:fusion} explains the fusion optimization. We evaluate \tool in \ref{sec:evaluation} and present related work in section \ref{sec:related-work}. Discussion of our approach is presented in section \ref{sec:discussion}, future work in section \ref{sec:future-work} and conclusions in section \ref{sec:conclusion}.
