\section{Introduction}
\label{sec:introduction}

% Motivation -- three different types programming models and their problems
In the past decade numerous systems for big data cluster computing have been studied \cite{dean_mapreduce:_2008, yu_dryadlinq:_2008-1, olston_pig_2008-1, thusoo_hive_2010-1, zaharia_spark:_2010}. Programming models of these systems impose a trade-off between generality, performance and productivity. Systems like Hadoop \cite{_hadoop_????} provide a low level general purpose programming model that allows writing fine grained and optimized code.  However, low level optimizations greatly sacrifice productivity \cite{chambers_flumejava:_2010}. Limited programming models like Pig latin \cite{olston_pig_2008-1} exploit domain knowledge to provide both good performance and productivity for large number of use cases. However, they support generality only through user defined functions that are cumbersome and difficult to optimize. Finally, models like Spark \cite{zaharia_spark:_2010}, FlumeJava \cite{chambers_flumejava:_2010} and Dryad/LINQ \cite{yu_dryadlinq:_2008-1} provide high level operations and general purpose programming models but their performance is limited by glue code between high level operations. Also, many relational optimizations are impossible due to the lack of knowledge about the program structure. 

% Trade-off - why cant there be both generality, performance and productivity
Above mentioned trade-off exists due to the imperative programming models, run-time binding of methods and open world assumptions commonly used in languages for big data processing. This limits compiler's ability to optimize the code and inefficient abstractions like iterators and channels are not removed from the code that connects declarative operations. Side effect free operations like date/time instantiation, regular expression compilation and high precision decimal numbers are often hidden by abstraction and recomputed in the hot path for each piece of data. Although domain specific approaches like Pig and SQL, with a narrow and side effect free interface, provide good optimizations they come with their own set of limitations. Their programming model is often too simple for the problem, leading to either invoking host language operations which are again hard to optimize or to abandoning the model completely. Moreover, there is the high overhead of learning the new language and tools. With these approaches the proper tool support in terms of debugging, type driven correctness checking and IDE is often limited.     

% Existing Solutions to achieve this
In the recent history there have been several solutions that try to make programming big data systems efficient, productive and general at the same time. Steno \cite{murray_steno:_2011}, implements an innovative runtime code generation scheme that aims to eliminate iterators in Dryad/LINQ queries. It operates over flat and nested queries and produces a minimal number of while loops without any iterator calls. Manimal \cite{jahani_automatic_2011} and HadoopToSQL \cite{iu_hadooptosql:_2010} apply byte code analysis to extract information about accessed fields and selection branches to gain enough knowledge to apply common relational database optimizations for projection and selection. However, since these solutions use static byte code analysis they must be safely conservative which can lead to missed optimization opportunities.         

% Our implementation
This paper presents a new domain specific language \tool for big data processing that provides a high level interface similar to the Spark framework. \tool uses language virtualization \cite{moors_scala-virtualized_2012} and lightweight modular staging (LMS)\cite{rompf_lightweight_2010} to get runtime access to the program structure. LMS enforces explicit effects tracking and provides modules for majority of language constructs and often used libraries making \tool more general than Pig and SQL. \tool uses the program structure to apply first modular framework specific optimizations and than common ones like code motion, loop fusion, projection optimization and inlining. Result is a highly optimized intermediate representation (IR) which is then used by a thin code generation layer to produce code for multiple big data back-end. Having common optimizations separated from code generation layer makes supporting new backends  easy. This makes the user code easily portable between frameworks and their versions. We implemented back-ends for Spark, Scoobi \cite{nicta_scoobi_2012} and Crunch \cite{_crunch_2012} and code generation layers range from 270 to 425 lines of code in size. 

\todo{evaluation paragraph after we make points there more clear}

% Contributions
\tool makes following contributions to the state of the art:    
\begin{itemize}

  \item We implement the \tool framework for big data processing. \tool has a high level programming model that applies relational and domain specific optimizations but does not restrict the language and has a minimal performance penalty.

  \item We introduce a novel projection insertion algorithm that operates across general program constructs like classes, conditionals, loops and user defined functions, takes the whole program into account and does not rely on safe assumptions which lead to missed optimization opportunities.  

  \item We show that language virtualization and lightweight modular staging allows easy extension and code portability for big data frameworks. 
\todo{need to link this to above statements which somehow felt out of the story.}
\end{itemize} 

% Sections
In section \ref{sec:background} we will provide background on LMS and language virtualization. Then, in section \ref{sec:programming-model} we explain the programming model and present simple program examples. Section \ref{sec:field-reduction} explains the projection insertion optimization. Section \ref{sec:fusion} explains the fusion optimization. We evaluate \tool in \ref{sec:evaluation} and present related work in section \ref{sec:related-work}. Discussion of our approach is presented in section \ref{sec:discussion}, future work in section \ref{sec:future-work} and conclusions in section \ref{sec:conclusion}.
